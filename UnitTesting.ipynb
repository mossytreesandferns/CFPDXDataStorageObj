{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnitTesting.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPlfeTb5/S+hewkClvrOphd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mossytreesandferns/CFPDXDataStorageObj/blob/master/UnitTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wB_L9J6bX9L",
        "colab_type": "code",
        "outputId": "2e5f78f0-9cdb-462d-d613-cb74ae156d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Mount my drive- run the code, go to the link, accept.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h5PEsV1bhFC",
        "colab_type": "code",
        "outputId": "57906117-8196-48e1-c037-82323a8aa493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Change working directory to make it easier to access the files\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/DCCleaningData/\")\n",
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/DCCleaningData'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkt24B0bbh85",
        "colab_type": "text"
      },
      "source": [
        "# Unit Testing Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk_i8mHhbjGk",
        "colab_type": "text"
      },
      "source": [
        "### Why unit test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1SCw14Rhu0Q",
        "colab_type": "text"
      },
      "source": [
        "### Write a simple unit test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTfn-8Zxbju2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pytest, unittest, nosetests, doctest are all python unittest libraries\n",
        "\n",
        "# create test_function_name.py\n",
        "\n",
        "# in test_function_name.py:\n",
        "\n",
        "import pytest\n",
        "import function_name\n",
        "\n",
        "def test_function_name_right():\n",
        "  assert  function_name(clean_parameters)\n",
        "  # every test has to have an assert statement # boolean expression\n",
        "          # returns nothing because true if true, returns assertion error if fail\n",
        "\n",
        "# def test_functions_expected_errors():\n",
        "\n",
        "def test_function_name_none_error():\n",
        "  assert function_name(incorrect_parameters) is None\n",
        "\n",
        "def test_function_name_missing_tab_errors():\n",
        "  assert function_name(incorrect_parameters) is None # is precedes None, == recedes numerical values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nw72VIb3fFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Go to command line\n",
        "\n",
        "pytest test_function_name.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEQHtXLzJxwW",
        "colab_type": "text"
      },
      "source": [
        "### The Test Result Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbeM5kK1Jztt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_TO1d43YvsG",
        "colab_type": "text"
      },
      "source": [
        "# Intermediate Unit Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVD-2llCY1aj",
        "colab_type": "text"
      },
      "source": [
        "### Mastering Assert Statements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaq9PMCYY0eJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert boolean_expression, message # message with info about why error is raised gets printed when assertion error is raised (boolean is false)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-J-EUaYaGQJ",
        "colab_type": "text"
      },
      "source": [
        "Don't compare floats in assert statements because python has idiosyncratic way of computing float computation. Use pytest.approx()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Qx0av4aFWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# eg\n",
        "assert .1 + .1 + .1 + pytest.approx(.3) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBS6Pc2ydEkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest\n",
        "from preprocessing_helpers import convert_to_int\n",
        "\n",
        "def test_on_string_with_one_comma():\n",
        "    test_argument = \"2,081\"\n",
        "    expected = 2081\n",
        "    actual = convert_to_int(test_argument)\n",
        "    # Format the string with the actual return value\n",
        "    message = \"convert_to_int('2,081') should return the int 2081, but it actually returned {0}\".format(actual)\n",
        "    # Write the assert statement which prints message on failure\n",
        "    assert actual is expected, message"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr9hqryAesgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pytest\n",
        "from as_numpy import get_data_as_numpy_array\n",
        "\n",
        "def test_on_clean_file():\n",
        "  expected = np.array([[2081.0, 314942.0],\n",
        "                       [1059.0, 186606.0],\n",
        "  \t\t\t\t\t   [1148.0, 206186.0]\n",
        "                       ]\n",
        "                      )\n",
        "  actual = get_data_as_numpy_array(\"example_clean_data.txt\", num_columns=2)\n",
        "  message = \"Expected return value: {0}, Actual return value: {1}\".format(expected, actual)\n",
        "  # Complete the assert statement\n",
        "  assert actual == pytest.approx(expected), message"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2-EtBGgpcRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_on_six_rows():\n",
        "    example_argument = np.array([[2081.0, 314942.0], [1059.0, 186606.0],\n",
        "                                [1148.0, 206186.0], [1506.0, 248419.0],\n",
        "                                [1210.0, 214114.0], [1697.0, 277794.0]]\n",
        "                               )\n",
        "    # Fill in with training array's expected number of rows\n",
        "    expected_training_array_num_rows = 4\n",
        "     # Fill in with testing array's expected number of rows\n",
        "    expected_testing_array_num_rows = 2\n",
        "    # Checkign values according to index of returned tuples\n",
        "    assert actual[0].shape[0] == 4, \"The actual number of rows in the training array is not {}\".format(expected_training_array_num_rows)\n",
        "    assert actual[1].shape[1] == 2, \"The actual number of rows in the testing array is not {}\".format(expected_testing_array_num_rows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g__OJyYIq297",
        "colab_type": "text"
      },
      "source": [
        "### Testing for Exceptions instead of return values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkQ5AY9SwVU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using context managers to check if ValueError tests are working correctly, if not, raise Exception\n",
        "with pytest.raises(ValueError):\n",
        "  raise ValueError\n",
        "  # pytest raises value error then silences it\n",
        "\n",
        "with pytest.raises(ValueError):\n",
        "  pass # pytest raises exception  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjePKbXpxpQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_valuerror_on_one_d_array():\n",
        "  example_arg=np.array([1,2,3,4,5])\n",
        "  with pytest.raises(ValueError) as exception_info:\n",
        "    split_into_training_and_testing_sets(example_argument)\n",
        "    # if 1d array doesn't raise a ValueError, then the test will fail\n",
        "  assert exception_info.match(\"Argument must be two dimentional. Got one dimensional instead.\")  # .match() takes string and checks if it is in the error message"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvxbONk6z5XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with pytest.raises(ValueError) as exc_info:\n",
        "    raise ValueError(\"Silence me!\")\n",
        "# Check if the raised ValueError contains the correct message\n",
        "assert exc_info.match(\"Silence me!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX3jnmohKd9O",
        "colab_type": "text"
      },
      "source": [
        "### The Well Tested Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7U4aQoUKhMA",
        "colab_type": "text"
      },
      "source": [
        "Pick a few arguments from each of the following categories: <br>\n",
        "* bad arguments - raise exceptions (eg ValueError's for 1d array)\n",
        "* special arguments - boundary values, values for the particular logic of the function. (eg making function return 2x2 split of training and testing even though it should be 3x1 for a 3:1 ratio of traning to testing). 4 rows is a special case.\n",
        "* normal arguments - in the case of .75 train .25 test case, 1 row returns value error, 2,3,5 are boundary cases, 4 is a special case, 6-> are normal arguments. Test for 2 or 3 normal arguments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjdlYhDBSV93",
        "colab_type": "text"
      },
      "source": [
        "### Test Driven Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r80BTb42SwTz",
        "colab_type": "text"
      },
      "source": [
        "Write unit tests before functions implemented and then write functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7IfWQQaKgl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkR1IfgUVjP",
        "colab_type": "text"
      },
      "source": [
        "# Test Organization and Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmFW7En0UV93",
        "colab_type": "text"
      },
      "source": [
        "### How to Organize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDrDsHHIVvtk",
        "colab_type": "text"
      },
      "source": [
        "There is a 'tests' package on teh same level as the operating package with test docs mirroring the operating package docs. <br>\n",
        "All tests for a single function should be in their own class. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J2WugpxUd36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest\n",
        "import numpy as np\n",
        "\n",
        "from models.train import split_into_training_and_testing_sets\n",
        "\n",
        "# Declare the test class\n",
        "class TestSplitIntoTrainingAndTestingSets(object):\n",
        "    # Fill in with the correct mandatory argument\n",
        "    def test_on_one_row(self):\n",
        "        test_argument = np.array([[1382.0, 390167.0]])\n",
        "        with pytest.raises(ValueError) as exc_info:\n",
        "            split_into_training_and_testing_sets(test_argument)\n",
        "        expected_error_msg = \"Argument data_array must have at least 2 rows, it actually has just 1\"\n",
        "        assert exc_info.match(expected_error_msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaIMC8AyX9HC",
        "colab_type": "text"
      },
      "source": [
        "### Mastering Test Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrUcS5QqYAKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# command line code\n",
        "\n",
        "cd tests\n",
        "pytest # os runs all tests !pytest in a Jupyter Notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnCHihp4YwFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pytest -x # stops testing after 1st failure, run before committing?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wndJSBgZXl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# recall:\n",
        "pytest data/nameoftestingdoc.py # to run tests for only one function "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycqfPkvycroN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path to test module::ClassName is known as the 'node id'\n",
        "pytest data/nameoftestingdoc.py::'TestClassName'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc4IcS1wZ1uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# running tests with keywords -k:\n",
        "\n",
        "pytest -k 'TestClassName'\n",
        "\n",
        "# can specify not:\n",
        "\n",
        "pytest -k 'FirstPartofTestClassName and not test_function_name'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YnSmsJ_dZYG",
        "colab_type": "text"
      },
      "source": [
        "### Expected Failures and Conditional Skipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrUHjidMpHyp",
        "colab_type": "text"
      },
      "source": [
        "TEsts can fail if functions not written -false alarm.Use test fail decorator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ywCWbxAddDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest\n",
        "class TestClass(object):\n",
        "  @pytest.mark.fail\n",
        "  def test_class_function(self):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ylCjj2pk9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Failing tests under certain conditions\n",
        "import pytest\n",
        "class TestClass(object):\n",
        "  @pytest.mark.skipif(boolean_expression)  # eg if using python > 2.7, import sys and pass in sys.version_info >=(2,7)\n",
        "  def test_class_function(self):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap7Vz4pDqdu1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to print reason for skipping in report\n",
        "\n",
        "import pytest -r[reasonfor skipping]\n",
        "class TestClass(object):\n",
        "  @pytest.mark.fail\n",
        "  def test_class_function(self):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K4AL7jRqd5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest -rs #  will show skipped tests in summary\n",
        "class TestClass(object):\n",
        "  @pytest.mark.fail\n",
        "  def test_class_function(self):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRMNIpm6rEN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest  #  \n",
        "class TestClass(object):\n",
        "  @pytest.mark.xfail(reason='type whatever reason you want')\n",
        "  def test_class_function(self):"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEaWfQUbrT_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pytest -rx #  will show entered  reasons in test summary\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBa7podPv-5Q",
        "colab_type": "text"
      },
      "source": [
        "### Continuous integration and code coverage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEOTM0O6xpxJ",
        "colab_type": "text"
      },
      "source": [
        "Build status badge: uses a an integration (eg Travis CI) server to run all tests every time there is a commit pushed to Github. <br>\n",
        "Create .travis.yml file (configuration file) at the same level as tests. Type this: <br>\n",
        "language: python<br>\n",
        "python:<br>\n",
        "  \\- '3.6'<br>\n",
        "\n",
        "install:<br>\n",
        "  \\- pip install -e  <br>\n",
        "\n",
        "script: <br>\n",
        "  \\- pytest tests<br>\n",
        "\n",
        "then push to githhub  git add travis.yml git push origin master. Go to github->marketplace->travis.ci install app\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2ICnWLyzv9_",
        "colab_type": "text"
      },
      "source": [
        "Code coverage badge: percentage of app code that gets run when the test suite is run. 75% and above is good.  Add to travis.yml file: <br>\n",
        "language: python<br>\n",
        "python:<br>\n",
        "  \\- '3.6'<br>\n",
        "\n",
        "install:<br>\n",
        "  \\- pip install -e  <br>\n",
        "  \\- pip install pytest-cov codecov\n",
        "script: <br>\n",
        "  \\- pytest --cov=src tests (note change)<br>\n",
        "after success: <br>\n",
        "  \\- codecov  <br> \n",
        "  Go to github->marketplace->codecov install app  pushes reports to codecov.io.  Go to badge page on github and paste markdown code to get badge on account page."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MiSOcE911NJ",
        "colab_type": "text"
      },
      "source": [
        "# Testing Models Plots and More"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I-aHU4t17TK",
        "colab_type": "text"
      },
      "source": [
        "### Beyond Assertion, set up and tear down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWeB-hAlyfb7",
        "colab_type": "text"
      },
      "source": [
        "Setting up: <br>\n",
        "1. preprocessing function: implements initial functions that filter data\n",
        "def preprocess(): predicated on a raw data file to start out with\n",
        "\n",
        "2. function that calls preprocessing function=<br>\n",
        "def test_on_raw_data(fixture):<br>\n",
        "    data= fixture <br>\n",
        "creates raw data file, then calls preprocessing function\n",
        "next uses context manager to check that preprocessign function contains desired output with assert statements. Next removes raw and clean data file to make way for next tests. <br>\n",
        "setup-->assert-->teardown<br>\n",
        "setup and teardown is contained in a fixture() function and is called in the test function.\n",
        "3. def fixture():<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERCSkPMCv98c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytest\n",
        "@pytest.fixture\n",
        "def fixture():\n",
        "  # implement setup actions\n",
        "  raw_path = 'raw.txt'\n",
        "  clean_path = 'clean.txt'\n",
        "  yield data\n",
        "  #implement teardown\n",
        "  os.remove(raw_path)\n",
        "  os.remove(clean_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7obOpB5t1gCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_on_raw_data(fixture):\n",
        "  data= fixture"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC-G9T0c2WuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using tempdir\n",
        "@pytest.fixture\n",
        "def fixture(tempdir)\n",
        "  raw_path = tempdir.join('raw.txt')\n",
        "  clean_path = tempdir.join('clean.txt')\n",
        "  #use context manager to write files\n",
        "  yield raw_path, clean_path\n",
        "\n",
        "  #no need for teardown code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_te0xTvsJty9",
        "colab_type": "text"
      },
      "source": [
        "### Mocking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F91NQT05KNmr",
        "colab_type": "text"
      },
      "source": [
        "Mocking - testting functions independent of dependencies <br>\n",
        "pytest-mock, unittest.mock<br>\n",
        "dependencies(original non-test functions) are replaced by a 'mock'<br>\n",
        "pass into fixture function <br>\n",
        "mocker.patch(data.preprocessing_helpers.row_to_list)<br>\n",
        "<br>\n",
        "def test_on_raw_data(raw_and_clean_data_file, mocker):<br>\n",
        "raw_path, clean_path = raw_and_clean_data_file<br>\n",
        "row_to_list_mocker_patch = mocker.patch(data.preprocessing_helpers.row_to_list, side_effect = row_to_list_bug_free)<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as3XgGtOJu5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytest-mock"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8KL0vDHKg2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from unittest import unittest.mock.MagicMock() or mocker.patch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QSLDa-PbhYe",
        "colab_type": "text"
      },
      "source": [
        "### Testing Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN4q1r7A2cc2",
        "colab_type": "text"
      },
      "source": [
        "Create testing functions that split, train and test data <br>\n",
        "use _,_,_, for variables you don't use in scikitlearn functions <br>\n",
        "Use well known data sets to compute return values for test cases <br>\n",
        "Use inequalities. These let you assert that the data have a slope > 0 <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoMsPBe18S0S",
        "colab_type": "text"
      },
      "source": [
        "### Testing Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6FlXjHC_Cyz",
        "colab_type": "text"
      },
      "source": [
        "Created testing fucntions that plot data. <br>\n",
        "use pytest-mpl to ignore os related differences. install with !pip <br>\n",
        "!pytest -k 'name_of_test_file' --mpl <br>\n",
        "pytest --mpl-generate-path /home/repl/workspace/project/tests/visualization/baseline -k \"test_plot_for_almost_linear_data\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A4AHlLg1-l6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}